<!DOCTYPE html>
<html>
<head>
  <title>AI Emotion Recognition</title>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous">
  <script src="https://docs.opencv.org/master/opencv.js"></script>
  <script src="https://docs.opencv.org/master/utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"> </script>
  <script src="src/js/index.js"></script>
</head>

<body class="bg-info">

  <div class = "container">
   <h1 class="center-block">Emotion Recognition - From Scratch! &#128528&#128512&#128542&#128551&#128561&#129314&#129324&#129320<a href='https://github.com/naamqul/emotion' target="_blank" rel="noopener noreferrer">(GitHub)</a></h1>
   <h5>*** it may take a few seconds to start predictions depending on your hardware - please only run on Desktop. Lower latency solutions in progress ***</h5>
   <canvas width = "640" height="480" id="canvas" class="center-block bg-dark">
    <video type = "hidden" width= "640" height="480" id="videoInput"></video>
  </canvas>
  <ul>
    <li>
        <h5><strong>methodology:</strong> multi-task transfer learning approach to predict valence [-1,1], arousal [-1, 1], and 8 emotion labels <em>[neutral, happy, sad, surprise, fear, disgust, anger, contempt]</em></h5>
        <ul>
            <li>
                <h5>valence + arousal only used for multi-task knowledge distillation purposes and not currently implemented in current predictions.</h5>
            </li>
        </ul>
    </li>
    <li>
        <h5><strong>neural network architecture</strong>: NasNetMobile base pretrained on imagenet followed by a shared fully-connected layer (dense0). following dense0, valence and arousal heads each have their own fully-connected layers with relu followed by a linear output layer. expression head has a skip connection combining the aforementioned dense layers followed by a softmax output for classification. see below for more information:</h5>
        <ul>
            <li>
                <h5>valence head: base ->  dense0 -> dense1 -> linear output</h5>
            </li>
            <li>
                <h5>arousal head: base -> dense0 -> dense2 -> linear output</h5>
            </li>
            <li>
                <h5>expression head: base -> [dense0 + concat(dense1, dense2)] -> softmax layer</h5>
            </li>
        </ul>
    </li>
    <li><h5>strong>data + training:</strong> <a href='http://mohammadmahoor.com/affectnet/'>AffectNet8</a> dataset was used for training (~290,000 Images) using an image generator for creating random augmentations (crop, shift, etc..). model was trained using 85% of training data. early stopping was used with a 15% validation set to prevent overfitting. first trained for 5 (+ 1  patience) epochs with a frozen base and then finetuned for 16 (+ 3 patience) epochs. model also used batch normalization, dropout, l2 regularization as well.</h5></li>
    <li>
        <h5><strong>deployment:</strong> keras model (~60mb) has been converted to a tensorflow js graph model (~10mb) with float16 quantization. website is in vanilla javascript and html. everything is running client-side and in browser so no data is collected (no backend server being used!). opencv was used for bounding boxes (haar cascade) to crop in on faces.</h5 >
    </li>
</ul>
</div>
<script type="text/javascript">openCVReady()</script>
</body>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha384-nvAa0+6Qg9clwYCGGPpDQLVpLNn0fRaROjHqs13t4Ggj3Ez50XnGQqc/r8MhnRDZ" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>
</html>