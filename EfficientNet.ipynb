{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f284c82c96b9b538\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f284c82c96b9b538\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs --port=6006 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_learning_curves, build_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = build_data('train')\n",
    "val = build_data('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_expression_output_loss', \n",
    "        patience = 3,\n",
    "        restore_best_weights = True\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'models/efficient.h5',\n",
    "        monitor=\"val_expression_output_loss\",\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode=\"auto\",\n",
    "        save_freq=\"epoch\",\n",
    "        period = 1\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(log_dir=\"logs/\", update_freq=3500)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244504 validated image filenames.\n",
      "Found 43147 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "generator = ImageDataGenerator(\n",
    "    rotation_range=10, # rotation\n",
    "    width_shift_range=0.2, # horizontal shift\n",
    "    height_shift_range=0.2, # vertical shift\n",
    "    zoom_range=0.2, # zoom\n",
    "    horizontal_flip=True, # horizontal flip\n",
    "    validation_split = .15\n",
    ")\n",
    "train_gen = generator.flow_from_dataframe(\n",
    "    dataframe = train,\n",
    "    x_col = 'image_path',\n",
    "    y_col = ['valence', 'arousal', 'expression'],\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'multi_output',\n",
    "    subset = 'training',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    preprocessing_function = keras.applications.efficientnet.preprocess_input,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_gen = generator.flow_from_dataframe(\n",
    "    dataframe = train,\n",
    "    x_col = 'image_path',\n",
    "    y_col = ['valence', 'arousal', 'expression'],\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'multi_output',\n",
    "    subset = 'validation',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    preprocessing_function = keras.applications.efficientnet.preprocess_input,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3999 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator().flow_from_dataframe(\n",
    "    dataframe = val,\n",
    "    x_col = 'image_path',\n",
    "    y_col = ['valence', 'arousal', 'expression'],\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'multi_output',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    preprocessing_function = keras.applications.efficientnet.preprocess_input,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_efficient():\n",
    "    inp = Input(input_shape)\n",
    "    \n",
    "    base = keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=inp,\n",
    "        pooling=None,\n",
    "    )\n",
    "    \n",
    "    base.trainable = False\n",
    "    \n",
    "    flatten = Flatten()(base.output)\n",
    "    \n",
    "    dense1 = Dense(256, kernel_regularizer = 'l2')(flatten)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dense1 = ReLU()(dense1)\n",
    "    dense1 = Dropout(.5)(dense1)\n",
    "\n",
    "    dense2 = Dense(128, kernel_regularizer = 'l2')(dense1)\n",
    "    dense2 = BatchNormalization()(dense2)\n",
    "    dense2 = ReLU()(dense2)\n",
    "    dense2 = Dropout(.5)(dense2)\n",
    "    \n",
    "    val_out = Dense(1, activation=None, name = 'valence_output')(dense2)\n",
    "    aro_out = Dense(1, activation=None, name = 'arousal_output')(dense2)\n",
    "    exp_out = Dense(8, activation=\"softmax\", name = 'expression_output')(dense2)\n",
    "    \n",
    "    model = keras.Model(inputs = inp, outputs = [val_out, aro_out, exp_out])\n",
    "    \n",
    "    return model, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16711680/16705208 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model, base = make_efficient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\", \"sparse_categorical_crossentropy\"],\n",
    "                  metrics = {\n",
    "                      'valence_output':[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                      'arousal_output':[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                      'expression_output':['accuracy']},\n",
    "                  optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7641/7641 [==============================] - 1708s 223ms/step - loss: 4.0602 - valence_output_loss: 0.2841 - arousal_output_loss: 0.1069 - expression_output_loss: 1.4386 - valence_output_root_mean_squared_error: 0.5315 - arousal_output_root_mean_squared_error: 0.3268 - expression_output_accuracy: 0.5039 - val_loss: 2.8854 - val_valence_output_loss: 0.2072 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 1.3223 - val_valence_output_root_mean_squared_error: 0.4552 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.5392\n",
      "Epoch 2/2\n",
      "7641/7641 [==============================] - 1702s 223ms/step - loss: 2.9001 - valence_output_loss: 0.2239 - arousal_output_loss: 0.1051 - expression_output_loss: 1.3475 - valence_output_root_mean_squared_error: 0.4732 - arousal_output_root_mean_squared_error: 0.3242 - expression_output_accuracy: 0.5250 - val_loss: 2.7280 - val_valence_output_loss: 0.2142 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 1.3199 - val_valence_output_root_mean_squared_error: 0.4628 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.5392\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        x = train_gen,\n",
    "        epochs = 2,\n",
    "        validation_data = val_gen,\n",
    "        verbose = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\", \"sparse_categorical_crossentropy\"],\n",
    "                  metrics = {\n",
    "                      'valence_output':[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                      'arousal_output':[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                      'expression_output':['accuracy']},\n",
    "                  optimizer=keras.optimizers.Adam(1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7641/7641 [==============================] - 1744s 228ms/step - loss: 2.0616 - valence_output_loss: 0.1819 - arousal_output_loss: 0.1053 - expression_output_loss: 1.1823 - valence_output_root_mean_squared_error: 0.4259 - arousal_output_root_mean_squared_error: 0.3246 - expression_output_accuracy: 0.5967 - val_loss: 1.3015 - val_valence_output_loss: 0.1362 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.9188 - val_valence_output_root_mean_squared_error: 0.3691 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.6878\n",
      "\n",
      "Epoch 00001: val_expression_output_loss improved from inf to 0.91882, saving model to models/efficient.h5\n",
      "Epoch 2/50\n",
      "7641/7641 [==============================] - 1738s 227ms/step - loss: 1.3161 - valence_output_loss: 0.1422 - arousal_output_loss: 0.1052 - expression_output_loss: 0.9482 - valence_output_root_mean_squared_error: 0.3771 - arousal_output_root_mean_squared_error: 0.3243 - expression_output_accuracy: 0.6802 - val_loss: 1.1541 - val_valence_output_loss: 0.1309 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.8285 - val_valence_output_root_mean_squared_error: 0.3617 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7233\n",
      "\n",
      "Epoch 00002: val_expression_output_loss improved from 0.91882 to 0.82847, saving model to models/efficient.h5\n",
      "Epoch 3/50\n",
      "7641/7641 [==============================] - 1735s 227ms/step - loss: 1.2032 - valence_output_loss: 0.1379 - arousal_output_loss: 0.1046 - expression_output_loss: 0.8760 - valence_output_root_mean_squared_error: 0.3714 - arousal_output_root_mean_squared_error: 0.3234 - expression_output_accuracy: 0.7070 - val_loss: 1.0907 - val_valence_output_loss: 0.1291 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.7796 - val_valence_output_root_mean_squared_error: 0.3593 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7355\n",
      "\n",
      "Epoch 00003: val_expression_output_loss improved from 0.82847 to 0.77965, saving model to models/efficient.h5\n",
      "Epoch 4/50\n",
      "7641/7641 [==============================] - 1737s 227ms/step - loss: 1.1509 - valence_output_loss: 0.1356 - arousal_output_loss: 0.1047 - expression_output_loss: 0.8359 - valence_output_root_mean_squared_error: 0.3682 - arousal_output_root_mean_squared_error: 0.3235 - expression_output_accuracy: 0.7197 - val_loss: 1.0582 - val_valence_output_loss: 0.1276 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.7537 - val_valence_output_root_mean_squared_error: 0.3572 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7429\n",
      "\n",
      "Epoch 00004: val_expression_output_loss improved from 0.77965 to 0.75375, saving model to models/efficient.h5\n",
      "Epoch 5/50\n",
      "7641/7641 [==============================] - 1737s 227ms/step - loss: 1.1172 - valence_output_loss: 0.1335 - arousal_output_loss: 0.1056 - expression_output_loss: 0.8080 - valence_output_root_mean_squared_error: 0.3654 - arousal_output_root_mean_squared_error: 0.3249 - expression_output_accuracy: 0.7282 - val_loss: 1.0371 - val_valence_output_loss: 0.1268 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.7365 - val_valence_output_root_mean_squared_error: 0.3561 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7465\n",
      "\n",
      "Epoch 00005: val_expression_output_loss improved from 0.75375 to 0.73649, saving model to models/efficient.h5\n",
      "Epoch 6/50\n",
      "7641/7641 [==============================] - 1738s 227ms/step - loss: 1.0949 - valence_output_loss: 0.1317 - arousal_output_loss: 0.1055 - expression_output_loss: 0.7904 - valence_output_root_mean_squared_error: 0.3629 - arousal_output_root_mean_squared_error: 0.3248 - expression_output_accuracy: 0.7332 - val_loss: 1.0208 - val_valence_output_loss: 0.1266 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.7217 - val_valence_output_root_mean_squared_error: 0.3558 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7493\n",
      "\n",
      "Epoch 00006: val_expression_output_loss improved from 0.73649 to 0.72175, saving model to models/efficient.h5\n",
      "Epoch 7/50\n",
      "7641/7641 [==============================] - 1737s 227ms/step - loss: 1.0726 - valence_output_loss: 0.1314 - arousal_output_loss: 0.1048 - expression_output_loss: 0.7705 - valence_output_root_mean_squared_error: 0.3625 - arousal_output_root_mean_squared_error: 0.3237 - expression_output_accuracy: 0.7398 - val_loss: 1.0054 - val_valence_output_loss: 0.1261 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.7093 - val_valence_output_root_mean_squared_error: 0.3551 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7537\n",
      "\n",
      "Epoch 00007: val_expression_output_loss improved from 0.72175 to 0.70932, saving model to models/efficient.h5\n",
      "Epoch 8/50\n",
      "7641/7641 [==============================] - 1737s 227ms/step - loss: 1.0611 - valence_output_loss: 0.1305 - arousal_output_loss: 0.1054 - expression_output_loss: 0.7613 - valence_output_root_mean_squared_error: 0.3612 - arousal_output_root_mean_squared_error: 0.3247 - expression_output_accuracy: 0.7421 - val_loss: 0.9940 - val_valence_output_loss: 0.1253 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.7002 - val_valence_output_root_mean_squared_error: 0.3539 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7574\n",
      "\n",
      "Epoch 00008: val_expression_output_loss improved from 0.70932 to 0.70022, saving model to models/efficient.h5\n",
      "Epoch 9/50\n",
      "7641/7641 [==============================] - 1736s 227ms/step - loss: 1.0452 - valence_output_loss: 0.1300 - arousal_output_loss: 0.1045 - expression_output_loss: 0.7480 - valence_output_root_mean_squared_error: 0.3606 - arousal_output_root_mean_squared_error: 0.3233 - expression_output_accuracy: 0.7462 - val_loss: 0.9886 - val_valence_output_loss: 0.1251 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6957 - val_valence_output_root_mean_squared_error: 0.3536 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7573\n",
      "\n",
      "Epoch 00009: val_expression_output_loss improved from 0.70022 to 0.69570, saving model to models/efficient.h5\n",
      "Epoch 10/50\n",
      "7641/7641 [==============================] - 1736s 227ms/step - loss: 1.0321 - valence_output_loss: 0.1286 - arousal_output_loss: 0.1049 - expression_output_loss: 0.7367 - valence_output_root_mean_squared_error: 0.3586 - arousal_output_root_mean_squared_error: 0.3239 - expression_output_accuracy: 0.7507 - val_loss: 0.9842 - val_valence_output_loss: 0.1257 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6913 - val_valence_output_root_mean_squared_error: 0.3545 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7579\n",
      "\n",
      "Epoch 00010: val_expression_output_loss improved from 0.69570 to 0.69129, saving model to models/efficient.h5\n",
      "Epoch 11/50\n",
      "7641/7641 [==============================] - 1737s 227ms/step - loss: 1.0290 - valence_output_loss: 0.1287 - arousal_output_loss: 0.1057 - expression_output_loss: 0.7331 - valence_output_root_mean_squared_error: 0.3588 - arousal_output_root_mean_squared_error: 0.3251 - expression_output_accuracy: 0.7512 - val_loss: 0.9782 - val_valence_output_loss: 0.1251 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6863 - val_valence_output_root_mean_squared_error: 0.3537 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7585\n",
      "\n",
      "Epoch 00011: val_expression_output_loss improved from 0.69129 to 0.68629, saving model to models/efficient.h5\n",
      "Epoch 12/50\n",
      "7641/7641 [==============================] - 1734s 227ms/step - loss: 1.0132 - valence_output_loss: 0.1281 - arousal_output_loss: 0.1056 - expression_output_loss: 0.7187 - valence_output_root_mean_squared_error: 0.3579 - arousal_output_root_mean_squared_error: 0.3249 - expression_output_accuracy: 0.7541 - val_loss: 0.9745 - val_valence_output_loss: 0.1248 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6834 - val_valence_output_root_mean_squared_error: 0.3533 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7600\n",
      "\n",
      "Epoch 00012: val_expression_output_loss improved from 0.68629 to 0.68342, saving model to models/efficient.h5\n",
      "Epoch 13/50\n",
      "7641/7641 [==============================] - 1735s 227ms/step - loss: 1.0085 - valence_output_loss: 0.1272 - arousal_output_loss: 0.1055 - expression_output_loss: 0.7157 - valence_output_root_mean_squared_error: 0.3566 - arousal_output_root_mean_squared_error: 0.3248 - expression_output_accuracy: 0.7565 - val_loss: 0.9670 - val_valence_output_loss: 0.1247 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6765 - val_valence_output_root_mean_squared_error: 0.3532 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7607\n",
      "\n",
      "Epoch 00013: val_expression_output_loss improved from 0.68342 to 0.67651, saving model to models/efficient.h5\n",
      "Epoch 14/50\n",
      "7641/7641 [==============================] - 1734s 227ms/step - loss: 1.0005 - valence_output_loss: 0.1273 - arousal_output_loss: 0.1049 - expression_output_loss: 0.7083 - valence_output_root_mean_squared_error: 0.3568 - arousal_output_root_mean_squared_error: 0.3239 - expression_output_accuracy: 0.7576 - val_loss: 0.9640 - val_valence_output_loss: 0.1242 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6744 - val_valence_output_root_mean_squared_error: 0.3524 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7622\n",
      "\n",
      "Epoch 00014: val_expression_output_loss improved from 0.67651 to 0.67435, saving model to models/efficient.h5\n",
      "Epoch 15/50\n",
      "7641/7641 [==============================] - 1734s 227ms/step - loss: 0.9950 - valence_output_loss: 0.1266 - arousal_output_loss: 0.1049 - expression_output_loss: 0.7038 - valence_output_root_mean_squared_error: 0.3557 - arousal_output_root_mean_squared_error: 0.3238 - expression_output_accuracy: 0.7606 - val_loss: 0.9599 - val_valence_output_loss: 0.1242 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6698 - val_valence_output_root_mean_squared_error: 0.3524 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7642\n",
      "\n",
      "Epoch 00015: val_expression_output_loss improved from 0.67435 to 0.66985, saving model to models/efficient.h5\n",
      "Epoch 16/50\n",
      "7641/7641 [==============================] - 1734s 227ms/step - loss: 0.9872 - valence_output_loss: 0.1269 - arousal_output_loss: 0.1055 - expression_output_loss: 0.6951 - valence_output_root_mean_squared_error: 0.3562 - arousal_output_root_mean_squared_error: 0.3248 - expression_output_accuracy: 0.7639 - val_loss: 0.9586 - val_valence_output_loss: 0.1243 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6691 - val_valence_output_root_mean_squared_error: 0.3526 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7651\n",
      "\n",
      "Epoch 00016: val_expression_output_loss improved from 0.66985 to 0.66910, saving model to models/efficient.h5\n",
      "Epoch 17/50\n",
      "7641/7641 [==============================] - 1733s 227ms/step - loss: 0.9766 - valence_output_loss: 0.1261 - arousal_output_loss: 0.1045 - expression_output_loss: 0.6866 - valence_output_root_mean_squared_error: 0.3550 - arousal_output_root_mean_squared_error: 0.3233 - expression_output_accuracy: 0.7664 - val_loss: 0.9579 - val_valence_output_loss: 0.1242 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6684 - val_valence_output_root_mean_squared_error: 0.3524 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7640\n",
      "\n",
      "Epoch 00017: val_expression_output_loss improved from 0.66910 to 0.66835, saving model to models/efficient.h5\n",
      "Epoch 18/50\n",
      "7641/7641 [==============================] - 1736s 227ms/step - loss: 0.9771 - valence_output_loss: 0.1261 - arousal_output_loss: 0.1050 - expression_output_loss: 0.6864 - valence_output_root_mean_squared_error: 0.3551 - arousal_output_root_mean_squared_error: 0.3241 - expression_output_accuracy: 0.7643 - val_loss: 0.9574 - val_valence_output_loss: 0.1243 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6679 - val_valence_output_root_mean_squared_error: 0.3526 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7657\n",
      "\n",
      "Epoch 00018: val_expression_output_loss improved from 0.66835 to 0.66789, saving model to models/efficient.h5\n",
      "Epoch 19/50\n",
      "7641/7641 [==============================] - 1735s 227ms/step - loss: 0.9746 - valence_output_loss: 0.1256 - arousal_output_loss: 0.1049 - expression_output_loss: 0.6846 - valence_output_root_mean_squared_error: 0.3545 - arousal_output_root_mean_squared_error: 0.3239 - expression_output_accuracy: 0.7669 - val_loss: 0.9554 - val_valence_output_loss: 0.1240 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6669 - val_valence_output_root_mean_squared_error: 0.3521 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7648\n",
      "\n",
      "Epoch 00019: val_expression_output_loss improved from 0.66789 to 0.66693, saving model to models/efficient.h5\n",
      "Epoch 20/50\n",
      "7641/7641 [==============================] - 1736s 227ms/step - loss: 0.9638 - valence_output_loss: 0.1239 - arousal_output_loss: 0.1047 - expression_output_loss: 0.6764 - valence_output_root_mean_squared_error: 0.3520 - arousal_output_root_mean_squared_error: 0.3236 - expression_output_accuracy: 0.7689 - val_loss: 0.9525 - val_valence_output_loss: 0.1237 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6638 - val_valence_output_root_mean_squared_error: 0.3517 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7662\n",
      "\n",
      "Epoch 00020: val_expression_output_loss improved from 0.66693 to 0.66376, saving model to models/efficient.h5\n",
      "Epoch 21/50\n",
      "7641/7641 [==============================] - 1734s 227ms/step - loss: 0.9584 - valence_output_loss: 0.1239 - arousal_output_loss: 0.1049 - expression_output_loss: 0.6702 - valence_output_root_mean_squared_error: 0.3520 - arousal_output_root_mean_squared_error: 0.3239 - expression_output_accuracy: 0.7713 - val_loss: 0.9520 - val_valence_output_loss: 0.1245 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6627 - val_valence_output_root_mean_squared_error: 0.3528 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7673\n",
      "\n",
      "Epoch 00021: val_expression_output_loss improved from 0.66376 to 0.66272, saving model to models/efficient.h5\n",
      "Epoch 22/50\n",
      "7641/7641 [==============================] - 1733s 227ms/step - loss: 0.9553 - valence_output_loss: 0.1239 - arousal_output_loss: 0.1052 - expression_output_loss: 0.6672 - valence_output_root_mean_squared_error: 0.3520 - arousal_output_root_mean_squared_error: 0.3243 - expression_output_accuracy: 0.7717 - val_loss: 0.9555 - val_valence_output_loss: 0.1246 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6661 - val_valence_output_root_mean_squared_error: 0.3530 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7660\n",
      "\n",
      "Epoch 00022: val_expression_output_loss did not improve from 0.66272\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7641/7641 [==============================] - 1738s 227ms/step - loss: 0.9466 - valence_output_loss: 0.1232 - arousal_output_loss: 0.1050 - expression_output_loss: 0.6592 - valence_output_root_mean_squared_error: 0.3511 - arousal_output_root_mean_squared_error: 0.3241 - expression_output_accuracy: 0.7744 - val_loss: 0.9518 - val_valence_output_loss: 0.1242 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6625 - val_valence_output_root_mean_squared_error: 0.3524 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7655\n",
      "\n",
      "Epoch 00023: val_expression_output_loss improved from 0.66272 to 0.66254, saving model to models/efficient.h5\n",
      "Epoch 24/50\n",
      "7641/7641 [==============================] - 1736s 227ms/step - loss: 0.9483 - valence_output_loss: 0.1243 - arousal_output_loss: 0.1052 - expression_output_loss: 0.6596 - valence_output_root_mean_squared_error: 0.3526 - arousal_output_root_mean_squared_error: 0.3243 - expression_output_accuracy: 0.7741 - val_loss: 0.9533 - val_valence_output_loss: 0.1241 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6641 - val_valence_output_root_mean_squared_error: 0.3522 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7680\n",
      "\n",
      "Epoch 00024: val_expression_output_loss did not improve from 0.66254\n",
      "Epoch 25/50\n",
      "7641/7641 [==============================] - 1738s 227ms/step - loss: 0.9390 - valence_output_loss: 0.1242 - arousal_output_loss: 0.1047 - expression_output_loss: 0.6511 - valence_output_root_mean_squared_error: 0.3524 - arousal_output_root_mean_squared_error: 0.3235 - expression_output_accuracy: 0.7774 - val_loss: 0.9496 - val_valence_output_loss: 0.1239 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6611 - val_valence_output_root_mean_squared_error: 0.3520 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7668\n",
      "\n",
      "Epoch 00025: val_expression_output_loss improved from 0.66254 to 0.66112, saving model to models/efficient.h5\n",
      "Epoch 26/50\n",
      "7641/7641 [==============================] - 1739s 228ms/step - loss: 0.9384 - valence_output_loss: 0.1232 - arousal_output_loss: 0.1052 - expression_output_loss: 0.6513 - valence_output_root_mean_squared_error: 0.3510 - arousal_output_root_mean_squared_error: 0.3244 - expression_output_accuracy: 0.7776 - val_loss: 0.9501 - val_valence_output_loss: 0.1243 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6615 - val_valence_output_root_mean_squared_error: 0.3525 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7673\n",
      "\n",
      "Epoch 00026: val_expression_output_loss did not improve from 0.66112\n",
      "Epoch 27/50\n",
      "7641/7641 [==============================] - 1739s 228ms/step - loss: 0.9318 - valence_output_loss: 0.1225 - arousal_output_loss: 0.1051 - expression_output_loss: 0.6452 - valence_output_root_mean_squared_error: 0.3500 - arousal_output_root_mean_squared_error: 0.3241 - expression_output_accuracy: 0.7795 - val_loss: 0.9506 - val_valence_output_loss: 0.1240 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6612 - val_valence_output_root_mean_squared_error: 0.3521 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7668\n",
      "\n",
      "Epoch 00027: val_expression_output_loss did not improve from 0.66112\n",
      "Epoch 28/50\n",
      "7641/7641 [==============================] - 1737s 227ms/step - loss: 0.9318 - valence_output_loss: 0.1232 - arousal_output_loss: 0.1052 - expression_output_loss: 0.6437 - valence_output_root_mean_squared_error: 0.3510 - arousal_output_root_mean_squared_error: 0.3243 - expression_output_accuracy: 0.7800 - val_loss: 0.9533 - val_valence_output_loss: 0.1243 - val_arousal_output_loss: 0.1060 - val_expression_output_loss: 0.6641 - val_valence_output_root_mean_squared_error: 0.3525 - val_arousal_output_root_mean_squared_error: 0.3255 - val_expression_output_accuracy: 0.7674\n",
      "\n",
      "Epoch 00028: val_expression_output_loss did not improve from 0.66112\n"
     ]
    }
   ],
   "source": [
    "history_finetune = model.fit(\n",
    "        x = train_gen,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = val_gen,\n",
    "        callbacks = callbacks,\n",
    "        verbose = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f88603a3400>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1349/1349 [==============================] - 257s 190ms/step - loss: 0.9504 - valence_output_loss: 0.1239 - arousal_output_loss: 0.1060 - expression_output_loss: 0.6620 - valence_output_root_mean_squared_error: 0.3520 - arousal_output_root_mean_squared_error: 0.3255 - expression_output_accuracy: 0.7690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9503548741340637,\n",
       " 0.12387217581272125,\n",
       " 0.10596499592065811,\n",
       " 0.6619651317596436,\n",
       " 0.35195478796958923,\n",
       " 0.32552266120910645,\n",
       " 0.769045352935791]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/efficientB0_Affect.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
